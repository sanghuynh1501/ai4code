{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BERT_MODEL_PATH = 'microsoft/codebert-base'\n",
    "MARK_PATH = 'weights/model_markdown_07840.pth'\n",
    "CODE_PATH = 'weights/model_code.pth'\n",
    "CODE_MARK_PATH = 'weights/model_code_mark_07575.pth'\n",
    "CODE_MARK_RANK_PATH = 'weights/model_code_mark_rank.pth'\n",
    "SIGMOID_PATH = 'weights/model_sigmoid_40_mae.pth'\n",
    "FASTTEST_MODEL = 'weights/model140000.bin'\n",
    "DATA_DIR = Path('AI4Code')\n",
    "LABELS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'k',\n",
    "          'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "MAX_TREE_DEPTH = 8\n",
    "TREE_METHOD = 'gpu_hist'\n",
    "SUBSAMPLE = 0.6\n",
    "REGULARIZATION = 0.1\n",
    "GAMMA = 0.3\n",
    "POS_WEIGHT = 1\n",
    "EARLY_STOP = 50\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_TRAIN = 200\n",
    "RANK_COUNT = 20\n",
    "SIGMOID_RANK_COUNT = 10\n",
    "MD_MAX_LEN = 64\n",
    "CODE_MAX_LEN = 23\n",
    "TOTAL_MAX_LEN = 512\n",
    "MAX_LEN = 128\n",
    "NVALID = 0.1\n",
    "EPOCH = 5\n",
    "BS = 2\n",
    "NW = 1\n",
    "RANKS = [i for i in range(0, RANK_COUNT + 1, 1)]\n",
    "accumulation_steps = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "class MarkdownOnlyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MarkdownOnlyModel, self).__init__()\n",
    "        self.distill_bert = AutoModel.from_pretrained(BERT_MODEL_PATH)\n",
    "        self.top = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        x = self.distill_bert(ids, mask)[0]\n",
    "        x = self.top(x[:, 0, :])\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MarkdownRankModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MarkdownRankModel, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(BERT_MODEL_PATH)\n",
    "        self.top = nn.Linear(770, len(RANKS))\n",
    "        self.activation = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, ids, mask, fts, code_lens):\n",
    "        x = self.model(ids, mask)[0]\n",
    "        x = torch.cat((x[:, 0, :], fts, code_lens), 1)\n",
    "\n",
    "        x = self.top(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SigMoidModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigMoidModel, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(BERT_MODEL_PATH)\n",
    "        self.top = nn.Linear(770, 1)\n",
    "\n",
    "    def forward(self, ids, mask, fts, code_lens):\n",
    "        x = self.model(ids, mask)[0]\n",
    "        x = torch.cat((x[:, 0, :], fts, code_lens), 1)\n",
    "        x = self.top(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from bisect import bisect\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "from config import LABELS, RANK_COUNT, RANKS\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "stemmer = WordNetLemmatizer()\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "\n",
    "def id_to_label(ids):\n",
    "    return [LABELS.index(s) for s in ids]\n",
    "\n",
    "\n",
    "def label_to_id(labels):\n",
    "    return ''.join([LABELS[i] for i in labels])\n",
    "\n",
    "\n",
    "def preprocess_text(document):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(document))\n",
    "\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    # return document\n",
    "\n",
    "    # Lemmatization\n",
    "    tokens = document.split()\n",
    "    tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "    tokens = [word for word in tokens if len(word) > 3]\n",
    "\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "def preprocess_code(cell):\n",
    "    return str(cell).replace('\\\\n', '\\n')[:200]\n",
    "\n",
    "\n",
    "def read_notebook(path):\n",
    "    return (\n",
    "        pd.read_json(\n",
    "            path,\n",
    "            dtype={'cell_type': 'category', 'source': 'str'})\n",
    "        .assign(id=path.stem)\n",
    "        .rename_axis('cell_id')\n",
    "    )\n",
    "\n",
    "\n",
    "def get_features_mark(df, mode='train'):\n",
    "\n",
    "    features = []\n",
    "    df = df.sort_values('rank').reset_index(drop=True)\n",
    "\n",
    "    for _, sub_df in tqdm(df.groupby('id')):\n",
    "\n",
    "        mark_sub_df_all = sub_df[sub_df.cell_type == 'markdown']\n",
    "\n",
    "        for i in range(0, mark_sub_df_all.shape[0]):\n",
    "            mark = mark_sub_df_all.iloc[i]['cell_id']\n",
    "            pct_rank = mark_sub_df_all.iloc[i]['pct_rank']\n",
    "\n",
    "            feature = {\n",
    "                'mark': mark,\n",
    "                'pct_rank': pct_rank\n",
    "            }\n",
    "\n",
    "            features.append(feature)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_features_rank(df, mode='train'):\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    code_ranks = []\n",
    "    df = df.sort_values('rank').reset_index(drop=True)\n",
    "\n",
    "    for id, sub_df in tqdm(df.groupby('id')):\n",
    "\n",
    "        mark_sub_df_all = sub_df[sub_df.cell_type == 'markdown']\n",
    "        code_sub_df_all = sub_df[sub_df.cell_type == 'code']\n",
    "        total_code_len = len(code_sub_df_all)\n",
    "        total_md = mark_sub_df_all.shape[0]\n",
    "\n",
    "        for i in range(0, mark_sub_df_all.shape[0]):\n",
    "            for j in range(0, code_sub_df_all.shape[0], RANK_COUNT):\n",
    "                code_sub_df = code_sub_df_all[j: j + RANK_COUNT]\n",
    "\n",
    "                codes = code_sub_df['cell_id'].to_list()\n",
    "                ranks = code_sub_df['rank'].values\n",
    "                total_code = code_sub_df.shape[0]\n",
    "\n",
    "                mark = mark_sub_df_all.iloc[i]['cell_id']\n",
    "                rank = mark_sub_df_all.iloc[i]['rank']\n",
    "\n",
    "                min_rank = 0 if j == 0 else ranks[0]\n",
    "                max_rank = ranks[-1]\n",
    "\n",
    "                relative = 1\n",
    "\n",
    "                if total_code_len - j <= RANK_COUNT and rank > min_rank:\n",
    "                    relative = 1\n",
    "                else:\n",
    "                    if rank < min_rank or rank > max_rank:\n",
    "                        relative = 0\n",
    "\n",
    "                code_rank = 0\n",
    "                if relative == 1:\n",
    "                    if j == 0 and rank < ranks[0]:\n",
    "                        code_rank = 0\n",
    "                    else:\n",
    "                        sub_ranks = rank - ranks\n",
    "                        sub_ranks[sub_ranks < 0] = 100000\n",
    "                        code_rank = np.argmin(sub_ranks) + 1\n",
    "\n",
    "                if len(ranks) < RANK_COUNT:\n",
    "                    ranks = np.concatenate(\n",
    "                        [ranks, np.ones(RANK_COUNT - len(ranks),) * ranks[-1]], 0)\n",
    "\n",
    "                if mode == 'classification':\n",
    "                    if relative == 1:\n",
    "                        feature = {\n",
    "                            'id': id,\n",
    "                            'total_code': int(total_code),\n",
    "                            'total_md': int(total_md),\n",
    "                            'codes': codes,\n",
    "                            'ranks': ranks,\n",
    "                            'code_rank': code_rank,\n",
    "                            'mark': mark,\n",
    "                            'pct_rank': mark_sub_df_all.iloc[i]['pct_rank'],\n",
    "                            'relative': relative,\n",
    "                            'total_code_len': total_code_len\n",
    "                        }\n",
    "                        features.append(feature)\n",
    "                elif mode == 'sigmoid':\n",
    "                    if total_code_len > RANK_COUNT:\n",
    "                        feature = {\n",
    "                            'total_code': int(total_code),\n",
    "                            'total_md': int(total_md),\n",
    "                            'codes': codes,\n",
    "                            'ranks': ranks,\n",
    "                            'code_rank': code_rank,\n",
    "                            'mark': mark,\n",
    "                            'pct_rank': mark_sub_df_all.iloc[i]['pct_rank'],\n",
    "                            'relative': relative,\n",
    "                            'total_code_len': total_code_len\n",
    "                        }\n",
    "                        features.append(feature)\n",
    "                else:\n",
    "                    feature = {\n",
    "                        'total_code': int(total_code),\n",
    "                        'total_md': int(total_md),\n",
    "                        'codes': codes,\n",
    "                        'ranks': ranks,\n",
    "                        'code_rank': code_rank,\n",
    "                        'mark': mark,\n",
    "                        'pct_rank': mark_sub_df_all.iloc[i]['pct_rank'],\n",
    "                        'relative': relative,\n",
    "                        'total_code_len': total_code_len\n",
    "                    }\n",
    "                    features.append(feature)\n",
    "                labels.append(relative)\n",
    "                code_ranks.append(code_rank)\n",
    "\n",
    "    return np.array(features), np.array(labels), np.array(code_ranks)\n",
    "\n",
    "\n",
    "def validate_markdown(model, val_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    tbar = tqdm(val_loader, file=sys.stdout)\n",
    "\n",
    "    preds = []\n",
    "    mark_ids = []\n",
    "    mark_hash = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, mask, _, id) in enumerate(tbar):\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred = model(ids.to(device), mask.to(device))\n",
    "            preds += pred.detach().cpu().numpy().ravel().tolist()\n",
    "            mark_ids += [label_to_id(i) for i in id]\n",
    "\n",
    "    for mark, score in zip(mark_ids, preds):\n",
    "        mark_hash[mark] = score\n",
    "\n",
    "    return mark_hash\n",
    "\n",
    "\n",
    "def validate_sigmoid(model, val_loader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    tbar = tqdm(val_loader, file=sys.stdout)\n",
    "\n",
    "    total = 0\n",
    "    zero_total = 0\n",
    "    one_total = 0\n",
    "\n",
    "    total_true = 0\n",
    "    total_zero_true = 0\n",
    "    total_one_true = 0\n",
    "    relatives = []\n",
    "\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, mask, fts, _, code_lens, _, target, total_code_lens) in enumerate(tbar):\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred = model(ids.to(device), mask.to(device),\n",
    "                             fts.to(device), code_lens.to(device))\n",
    "\n",
    "            code_lens = (total_code_lens.detach().cpu().numpy().ravel()\n",
    "                         <= RANK_COUNT).astype(np.int8)\n",
    "            code_len_indexs = np.nonzero(code_lens == 1)[0]\n",
    "\n",
    "            pred = torch.sigmoid(pred)\n",
    "            pred = pred.detach().cpu().numpy().ravel()\n",
    "            pred[code_len_indexs] = 1.0\n",
    "            preds += pred.tolist()\n",
    "\n",
    "            pred = (pred >= threshold).astype(np.int8)\n",
    "            # pred = (pred | code_lens).astype(np.int8)\n",
    "            # pred = pred + code_lens\n",
    "            # pred = np.clip(pred, 0, 1)\n",
    "            relatives += pred.tolist()\n",
    "\n",
    "            target = target.detach().cpu().numpy().ravel()\n",
    "            targets += target.tolist()\n",
    "\n",
    "            zero_indexes = np.nonzero(target == 0)[0]\n",
    "            one_indexes = np.nonzero(target == 1)[0]\n",
    "\n",
    "            zero_target = target[zero_indexes]\n",
    "            one_target = target[one_indexes]\n",
    "\n",
    "            zero_pred = pred[zero_indexes]\n",
    "            one_pred = pred[one_indexes]\n",
    "\n",
    "            zero_total += len(zero_target)\n",
    "            one_total += len(one_target)\n",
    "            total += len(target)\n",
    "\n",
    "            total_zero_true += np.sum((zero_pred ==\n",
    "                                       zero_target).astype(np.int8))\n",
    "            total_one_true += np.sum((one_pred == one_target).astype(np.int8))\n",
    "            total_true += np.sum((pred == target).astype(np.int8))\n",
    "\n",
    "    return total_true / total, total_zero_true / zero_total, total_one_true / one_total, relatives, targets, preds\n",
    "\n",
    "\n",
    "def validate_rank_inference(model, val_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    tbar = tqdm(val_loader, file=sys.stdout)\n",
    "\n",
    "    preds = []\n",
    "    targets = []\n",
    "    mark_ids = []\n",
    "    mark_dict = {}\n",
    "    rank_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (ids, mask, fts, code_len, target, cell_id, ranks) in enumerate(tbar):\n",
    "            ranks = ranks.detach().cpu().numpy().tolist()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred = model(ids.to(device), mask.to(device),\n",
    "                             fts.to(device), code_len.to(device))\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "            preds.append(pred.detach().cpu().numpy().ravel())\n",
    "            targets.append(target.detach().cpu().numpy().ravel())\n",
    "            mark_ids += [label_to_id(i) for i in cell_id]\n",
    "            rank_list += ranks\n",
    "\n",
    "    preds, targets = np.concatenate(preds), np.concatenate(targets)\n",
    "\n",
    "    for (id, pred, rank) in zip(mark_ids, preds, rank_list):\n",
    "        if pred == 0:\n",
    "            mark_dict[id] = pred\n",
    "        else:\n",
    "            mark_dict[id] = rank[pred - 1] + 1\n",
    "\n",
    "    return preds, targets, accuracy_score(targets, preds), mark_dict\n",
    "\n",
    "\n",
    "def count_inversions(a):\n",
    "    inversions = 0\n",
    "    sorted_so_far = []\n",
    "    for i, u in enumerate(a):  # O(N)\n",
    "        j = bisect(sorted_so_far, u)  # O(log N)\n",
    "        inversions += i - j\n",
    "        sorted_so_far.insert(j, u)  # O(N)\n",
    "    return inversions\n",
    "\n",
    "\n",
    "def kendall_tau(ground_truth, predictions):\n",
    "    total_inversions = 0  # total inversions in predicted ranks across all instances\n",
    "    total_2max = 0  # maximum possible inversions across all instances\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        # rank predicted order in terms of ground truth\n",
    "        ranks = [gt.index(x) for x in pred]\n",
    "        total_inversions += count_inversions(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return 1 - 4 * total_inversions / total_2max\n",
    "\n",
    "\n",
    "def cal_kendall_tau_inference(df, mark_dict, final_pred, df_orders):\n",
    "    df.loc[df['cell_type'] == 'code',\n",
    "           'pred'] = df[df.cell_type == 'code']['rank']\n",
    "\n",
    "    marks = df.loc[df['cell_type'] == 'markdown']['cell_id'].to_list()\n",
    "    for mark in marks:\n",
    "        if mark not in final_pred:\n",
    "            final_pred[mark] = mark_dict[mark]\n",
    "\n",
    "    pred = []\n",
    "    cell_ids = []\n",
    "    for cell_id in final_pred.keys():\n",
    "        cell_ids.append(cell_id)\n",
    "        pred.append(final_pred[cell_id])\n",
    "\n",
    "    df_markdown_pred = pd.DataFrame(list(zip(cell_ids, pred)), columns=[\n",
    "                                    'cell_id', 'markdown_pred'])\n",
    "    df = df.merge(df_markdown_pred, on=['cell_id'], how='outer')\n",
    "\n",
    "    df.loc[df['cell_type'] == 'markdown',\n",
    "           'pred'] = df.loc[df['cell_type'] == 'markdown']['markdown_pred']\n",
    "\n",
    "    df[['id', 'cell_id', 'cell_type', 'rank', 'pred']].to_csv('predict.csv')\n",
    "    y_dummy = df.sort_values(\"pred\").groupby('id')['cell_id'].apply(list)\n",
    "    print(\"Preds score\", kendall_tau(df_orders.loc[y_dummy.index], y_dummy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class MarkdownOnlyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, fts, dict_cellid_source, max_len):\n",
    "        super().__init__()\n",
    "        self.dict_cellid_source = dict_cellid_source\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_PATH)\n",
    "        self.fts = fts\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.fts[index]\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            self.dict_cellid_source[row['mark']],\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = torch.LongTensor(inputs['input_ids'])\n",
    "        mask = torch.LongTensor(inputs['attention_mask'])\n",
    "\n",
    "        return ids, mask, torch.FloatTensor([row['pct_rank']]), torch.LongTensor(id_to_label(row['mark']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fts)\n",
    "\n",
    "\n",
    "class MarkdownRankNewDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dict_cellid_source, total_max_len, md_max_len, fts):\n",
    "        super().__init__()\n",
    "        self.dict_cellid_source = dict_cellid_source\n",
    "        self.md_max_len = md_max_len\n",
    "        self.total_max_len = total_max_len  # maxlen allowed by model config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_PATH)\n",
    "        self.fts = fts\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.fts[index]\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            self.dict_cellid_source[row['mark']],\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.md_max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        codes = row['codes']\n",
    "        ranks = row['ranks']\n",
    "        code_inputs = self.tokenizer.batch_encode_plus(\n",
    "            [str(self.dict_cellid_source[x]) for x in codes],\n",
    "            add_special_tokens=True,\n",
    "            max_length=CODE_MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        n_md = row['total_md']\n",
    "        n_code = row['total_code']\n",
    "        if n_md + n_code == 0:\n",
    "            fts = torch.FloatTensor([0])\n",
    "        else:\n",
    "            fts = torch.FloatTensor([n_md / (n_md + n_code)])\n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        for x in code_inputs['input_ids']:\n",
    "            ids.extend(x[:-1])\n",
    "        ids = ids[:self.total_max_len]\n",
    "        if len(ids) != self.total_max_len:\n",
    "            ids = ids + [self.tokenizer.pad_token_id, ] * \\\n",
    "                (self.total_max_len - len(ids))\n",
    "        ids = torch.LongTensor(ids)\n",
    "\n",
    "        mask = inputs['attention_mask']\n",
    "        for x in code_inputs['attention_mask']:\n",
    "            mask.extend(x[:-1])\n",
    "        mask = mask[:self.total_max_len]\n",
    "        if len(mask) != self.total_max_len:\n",
    "            mask = mask + [self.tokenizer.pad_token_id, ] * \\\n",
    "                (self.total_max_len - len(mask))\n",
    "        mask = torch.LongTensor(mask)\n",
    "\n",
    "        label = row['code_rank']\n",
    "\n",
    "        assert len(ids) == self.total_max_len\n",
    "\n",
    "        return ids, mask, fts, torch.FloatTensor([len(codes) / RANK_COUNT]), torch.LongTensor([label]), torch.LongTensor(id_to_label(row['mark'])), torch.LongTensor(ranks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fts)\n",
    "\n",
    "\n",
    "class SigMoidDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dict_cellid_source, total_max_len, md_max_len, fts):\n",
    "        super().__init__()\n",
    "        self.dict_cellid_source = dict_cellid_source\n",
    "        self.md_max_len = md_max_len\n",
    "        self.total_max_len = total_max_len  # maxlen allowed by model config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_PATH)\n",
    "        self.fts = fts\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.fts[index]\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            self.dict_cellid_source[row['mark']],\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.md_max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        codes = row['codes']\n",
    "        code_inputs = self.tokenizer.batch_encode_plus(\n",
    "            [str(self.dict_cellid_source[x]) for x in codes],\n",
    "            add_special_tokens=True,\n",
    "            max_length=CODE_MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        n_md = row['total_md']\n",
    "        n_code = row['total_code']\n",
    "        if n_md + n_code == 0:\n",
    "            fts = torch.FloatTensor([0])\n",
    "        else:\n",
    "            fts = torch.FloatTensor([n_md / (n_md + n_code)])\n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        for x in code_inputs['input_ids']:\n",
    "            ids.extend(x[:-1])\n",
    "        ids = ids[:self.total_max_len]\n",
    "        if len(ids) != self.total_max_len:\n",
    "            ids = ids + [self.tokenizer.pad_token_id, ] * \\\n",
    "                (self.total_max_len - len(ids))\n",
    "        ids = torch.LongTensor(ids)\n",
    "\n",
    "        mask = inputs['attention_mask']\n",
    "        for x in code_inputs['attention_mask']:\n",
    "            mask.extend(x[:-1])\n",
    "        mask = mask[:self.total_max_len]\n",
    "        if len(mask) != self.total_max_len:\n",
    "            mask = mask + [self.tokenizer.pad_token_id, ] * \\\n",
    "                (self.total_max_len - len(mask))\n",
    "        mask = torch.LongTensor(mask)\n",
    "\n",
    "        label = row['pct_rank']\n",
    "        relative = row['relative']\n",
    "        total_code_len = row['total_code_len']\n",
    "\n",
    "        loss_mask = torch.ones(RANK_COUNT + 1,)\n",
    "        loss_mask[:len(codes) + 1] = 0\n",
    "        loss_mask = loss_mask.type(torch.ByteTensor)\n",
    "\n",
    "        assert len(ids) == self.total_max_len\n",
    "\n",
    "        return ids, mask, fts, loss_mask, torch.FloatTensor([len(codes) / RANK_COUNT]), torch.FloatTensor([label]), torch.FloatTensor([relative]), torch.FloatTensor([total_code_len])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda'\n",
    "torch.cuda.empty_cache()\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = MarkdownRankModel()\n",
    "model.load_state_dict(torch.load(CODE_MARK_RANK_PATH))\n",
    "model = model.cuda()\n",
    "\n",
    "model_sigmoid = SigMoidModel().to(device)\n",
    "model_sigmoid.load_state_dict(torch.load(SIGMOID_PATH))\n",
    "model_sigmoid = model_sigmoid.cuda()\n",
    "\n",
    "model_mark_only = MarkdownOnlyModel()\n",
    "model_mark_only.load_state_dict(torch.load(MARK_PATH))\n",
    "model_mark_only = model_mark_only.cuda()\n",
    "\n",
    "paths_test = list((DATA_DIR / 'train').glob('*.json'))[-1000:]\n",
    "notebooks_test = [\n",
    "    read_notebook(path) for path in tqdm(paths_test, desc='Test NBs')\n",
    "]\n",
    "\n",
    "df = (\n",
    "    pd.concat(notebooks_test)\n",
    "    .set_index('id', append=True)\n",
    "    .swaplevel()\n",
    "    .sort_index(level='id', sort_remaining=False)\n",
    ")\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df_orders = pd.read_csv(\n",
    "    DATA_DIR / 'train_orders.csv',\n",
    "    index_col='id',\n",
    "    squeeze=True,\n",
    ").str.split()\n",
    "\n",
    "df.loc[df['cell_type'] == 'markdown', 'source'] = df[df['cell_type']\n",
    "                                                     == 'markdown'].source.apply(preprocess_text)\n",
    "\n",
    "df.loc[df['cell_type'] == 'code', 'source'] = df[df['cell_type']\n",
    "                                                 == 'code'].source.apply(preprocess_code)\n",
    "\n",
    "dict_cellid_source = dict(\n",
    "    zip(df['cell_id'].values, df['source'].values))\n",
    "\n",
    "df[\"rank\"] = df.groupby([\"id\", \"cell_type\"]).cumcount()\n",
    "df = df.sort_values('rank').reset_index(drop=True)\n",
    "df[\"pct_rank\"] = df[\"rank\"] / \\\n",
    "    df.groupby(\"id\")[\"cell_id\"].transform(\"count\")\n",
    "\n",
    "val_fts, _, _ = get_features_rank(df, 'test')\n",
    "val_fts_only = get_features_mark(df, 'test')\n",
    "\n",
    "val_ds = SigMoidDataset(dict_cellid_source, md_max_len=MD_MAX_LEN,\n",
    "                        total_max_len=512, fts=val_fts)\n",
    "val_ds_only = MarkdownOnlyDataset(val_fts_only, dict_cellid_source, 128)\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=BS * 8, shuffle=False, num_workers=NW,\n",
    "                        pin_memory=False, drop_last=False)\n",
    "val_loader_only = DataLoader(val_ds_only, batch_size=BS, shuffle=False, num_workers=NW,\n",
    "                             pin_memory=False, drop_last=False)\n",
    "\n",
    "acc, true, false, relative, _, _ = validate_sigmoid(\n",
    "    model_sigmoid, val_loader, device, 0.397705)\n",
    "print(acc, true, false)\n",
    "mark_dict = validate_markdown(model_mark_only, val_loader_only, device)\n",
    "\n",
    "class_fts = []\n",
    "one_object = {}\n",
    "mark_id_dict = {}\n",
    "for i in range(len(relative)):\n",
    "    if relative[i] == 1:\n",
    "        class_fts.append(val_fts[i])\n",
    "        if val_fts[i]['mark'] not in one_object:\n",
    "            one_object[val_fts[i]['mark']] = 1\n",
    "        else:\n",
    "            del one_object[val_fts[i]['mark']]\n",
    "\n",
    "for ft in val_fts:\n",
    "    if ft['mark'] not in one_object:\n",
    "        mark_id_dict[ft['mark']] = mark_dict[ft['mark']] * \\\n",
    "            (ft['total_code'] + ft['total_md'])\n",
    "\n",
    "val_ds = MarkdownRankNewDataset(dict_cellid_source, md_max_len=MD_MAX_LEN,\n",
    "                                total_max_len=512, fts=class_fts)\n",
    "val_loader = DataLoader(val_ds, batch_size=BS * 8, shuffle=False, num_workers=NW,\n",
    "                        pin_memory=False, drop_last=False)\n",
    "\n",
    "\n",
    "y_pred, _, acc, mark_dict = validate_rank_inference(model, val_loader, device)\n",
    "cal_kendall_tau_inference(df, mark_id_dict, mark_dict, df_orders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ai4code_new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "360d45463c6cb6b732bb0bfa29da53a9e6ed32c27f9fad1b14c8f5628a173e8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
